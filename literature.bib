% DQN

@Book{Sutton2018,
  author = {Richard S. Sutton and Andrew G. Barto},
  year = 2018,
  title = {Reinforcement Learning: An Introduction},
  series = {(Adaptive Computation and Machine Learning series)},
  edition = 2,
  publisher = {MIT Press},
  place = {Cambridge, MA},
  isbn = {978-0262039246}
}

@article{Garey1976,
 ISSN = {0364765X, 15265471},
 URL = {http://www.jstor.org/stable/3689278},
 abstract = {NP-complete problems form an extensive equivalence class of combinatorial problems for which no nonenumerative algorithms are known. Our first result shows that determining a shortest-length schedule in an m-machine flowshop is NP-complete for m ≥ 3. (For m = 2, there is an efficient algorithm for finding such schedules.) The second result shows that determining a minimum mean-flow-time schedule in an m-machine flowshop is NP-complete for every m ≥ 2. Finally we show that the shortest-length schedule problem for an m-machine jobshop is NP-complete for every m ≥ 2. Our results are strong in that they hold whether the problem size is measured by number of tasks, number of bits required to express the task lengths, or by the sum of the task lengths.},
 author = {M. R. Garey and D. S. Johnson and Ravi Sethi},
 journal = {Mathematics of Operations Research},
 number = {2},
 pages = {117--129},
 publisher = {INFORMS},
 title = {The Complexity of Flowshop and Jobshop Scheduling},
 volume = {1},
 year = {1976}
}

@mastersthesis{Hasenbichler2021,
  author  = {Timo Hasenbichler},
  title   = {Exploration of Deep Reinforcement Learning based approaches to job-shop scheduling problems using discrete event simulations},
  school  = {FH JOANNEUM -- University of Applied Sciences},
  year    = 2021
}

@article{Hornik1989,
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = 1989,
  title = {Multilayer Feedforward Networks are Universal Approximators},
  book = {Neural Networks},
  volume = 2,
  publisher = {Pergamon Press},
  page = {359–366}
}

% NN on salesman problem
@InProceedings{10.1007/BFb0032050,
author="Johnson, David S.",
editor="Paterson, Michael S.",
title="Local optimization and the Traveling Salesman Problem",
booktitle="Automata, Languages and Programming",
year="1990",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="446--461",
abstract="The Traveling Salesman Problem (TSP) is often cited as the prototypical ``hard'' combinatorial optimization problem. As such, it would seem to be an ideal candidate for nonstandard algorithmic approaches, such as simulated annealing, and, more recently, genetic algorithms. Both of these approaches can be viewed as variants on the traditional technique called local optimization. This paper surveys the state of the art with respect to the TSP, with emphasis on the performance of traditional local optimization algorithms and their new competitors, and on what insights complexity theory does, or does not, provide.",
isbn="978-3-540-47159-2"
}


% Hungarian method
@article{https://doi.org/10.1002/nav.3800020109,
author = {Kuhn, H. W.},
title = {The Hungarian method for the assignment problem},
journal = {Naval Research Logistics Quarterly},
volume = {2},
number = {1-2},
pages = {83-97},
doi = {https://doi.org/10.1002/nav.3800020109},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109},
abstract = {Abstract Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
year = {1955}
}

% Discrete event simulation
@article{osti_6893405,
title = {Principles of discrete event simulation. [Book review]},
author = {Fishman, G S},
abstractNote = {According to the author of the book, it is meant to describe concepts, methods, and procedures for modeling the behavior of a discrete event system, for translating the model into code executable on a digital computer, and for analyzing sample sequences that emerge from the program's execution. A revision of an earlier text, the current version incorporates many recent developments in discrete event simulation. In the opinion of the reviewer, the idea of the book is quite good, but the author is not successful in providing a text that clearly explains the recent developments. (RWR)},
doi = {},
url = {https://www.osti.gov/biblio/6893405}, journal = {},
place = {United States},
year = {1978},
month = {1}
}

% SimPy 
@article{matloff2008introduction,
  title={Introduction to discrete-event simulation and the simpy language},
  author={Matloff, Norm},
  journal={Davis, CA. Dept of Computer Science. University of California at Davis. Retrieved on August},
  volume={2},
  number={2009},
  pages={1--33},
  year={2008}
}

% Open AI Gym
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

% Open AI Gym environments
@online{openaigymenvs,
  author = {OpenAI},
  title = {Gym environments},
  url = {https://gym.openai.com/envs/},
  urldate = {2022-04-28}
}

% scipy
@Article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

% scipy linear sum assignment
@online{linearsumassignment,
  author = {SciPy},
  title = {scipy.optimize.linear\_sum\_assignment},
  url = {https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear\_sum\_assignment.html},
  urldate = {2022-04-28}
}

% Mayer
@Article{Mayer2021,
author={Mayer, Sebastian and Classen, Tobias and Endisch, Christian},
title={Modular production control using deep reinforcement learning: proximal policy optimization},
journal={Journal of Intelligent Manufacturing},
year={2021},
month={Dec},
day={01},
volume={32},
number={8},
pages={2335-2351},
abstract={EU regulations on {\$}{\$}{\backslash}textit{\{}CO{\}}{\_}2{\$}{\$}limits and the trend of individualization are pushing the automotive industry towards greater flexibility and robustness in production. One approach to address these challenges is modular production, where workstations are decoupled by automated guided vehicles, requiring new control concepts. Modular production control aims at throughput-optimal coordination of products, workstations, and vehicles. For this np-hard problem, conventional control approaches lack in computing efficiency, do not find optimal solutions, or are not generalizable. In contrast, Deep Reinforcement Learning offers powerful and generalizable algorithms, able to deal with varying environments and high complexity. One of these algorithms is Proximal Policy Optimization, which is used in this article to address modular production control. Experiments in several modular production control settings demonstrate stable, reliable, optimal, and generalizable learning behavior. The agent successfully adapts its strategies with respect to the given problem configuration. We explain how to get to this learning behavior, especially focusing on the agent's action, state, and reward design.},
issn={1572-8145},
doi={10.1007/s10845-021-01778-z},
url={https://doi.org/10.1007/s10845-021-01778-z}
}



@article{Mnih,
  added-at = {2021-01-06T14:52:16.000+0100},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin A. and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/27c6f54424f0d4672eae5411091b5bd95/frankyanpan},
  ee = {https://www.wikidata.org/entity/Q27907579},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {7c6f54424f0d4672eae5411091b5bd95},
  journal = {Nature},
  keywords = {Reinforcementlearning deeplearning},
  number = 7540,
  pages = {529-533},
  timestamp = {2021-01-06T14:52:16.000+0100},
  title = {Human-level control through deep reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/journals/nature/nature518.html#MnihKSRVBGRFOPB15},
  volume = 518,
  year = 2015
}

% PER
@misc{schaul2015prioritized,
  abstract = {Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. DQN with prioritized
experience replay achieves a new state-of-the-art, outperforming DQN with
uniform replay on 41 out of 49 games.},
  added-at = {2019-11-17T22:17:21.000+0100},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  biburl = {https://www.bibsonomy.org/bibtex/2be47ebfc4c5eb8f06b374fe632a3e236/jan.hofmann1},
  description = {[1511.05952] Prioritized Experience Replay},
  interhash = {db6e4402b0807938aae61afc45b70f73},
  intrahash = {be47ebfc4c5eb8f06b374fe632a3e236},
  keywords = {final reinforcement_learning thema:double_dqn},
  note = {cite arxiv:1511.05952Comment: Published at ICLR 2016},
  timestamp = {2019-12-09T10:13:58.000+0100},
  title = {Prioritized Experience Replay},
  url = {http://arxiv.org/abs/1511.05952},
  year = 2015
}

% DUELING
@article{WangFL15,
  author    = {Ziyu Wang and
               Nando de Freitas and
               Marc Lanctot},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06581},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06581},
  eprinttype = {arXiv},
  eprint    = {1511.06581},
  timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WangFL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Noisy
@article{APMOGM17,
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Ian Osband and
               Alex Graves and
               Vlad Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  title     = {Noisy Networks for Exploration},
  journal   = {CoRR},
  volume    = {abs/1706.10295},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.10295},
  eprinttype = {arXiv},
  eprint    = {1706.10295},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FortunatoAPMOGM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% DDQN
@article{HasseltGS15,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  archivePrefix = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}